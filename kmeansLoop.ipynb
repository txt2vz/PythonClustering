{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc9ec803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500 documents\n",
      "3 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sklearn\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans, MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_folder = \"C:/Data/DataSetForPaper2023/crisis3\"\n",
    "dataset = sklearn.datasets.load_files(data_folder,  description=None, categories=None, load_content=True, shuffle=True, encoding='utf-8', decode_error='ignore', random_state=0)\n",
    "\n",
    "print(\"%d documents\" % len(dataset.data))\n",
    "print(\"%d categories\" % len(dataset.target_names))\n",
    "print()\n",
    "labels = dataset.target\n",
    "true_k = np.unique(labels).shape[0]\n",
    "t0 = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dedf6eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf vectorizer\n",
      "done in 5.308848s\n",
      "n_samples: 1500, n_features: 1331\n",
      "kMeans ++ run number: 0===============================================\n",
      "Clustering sparse data with KMeans(max_iter=100, n_clusters=3, n_init='auto', verbose=False)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lauri\\OneDrive - Sheffield Hallam University\\Research\\Clustering\\Python\\kmeansLoop.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauri/OneDrive%20-%20Sheffield%20Hallam%20University/Research/Clustering/Python/kmeansLoop.ipynb#W1sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mClustering sparse data with \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m km)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauri/OneDrive%20-%20Sheffield%20Hallam%20University/Research/Clustering/Python/kmeansLoop.ipynb#W1sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m t0 \u001b[39m=\u001b[39m time()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lauri/OneDrive%20-%20Sheffield%20Hallam%20University/Research/Clustering/Python/kmeansLoop.ipynb#W1sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m km\u001b[39m.\u001b[39;49mfit(X)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauri/OneDrive%20-%20Sheffield%20Hallam%20University/Research/Clustering/Python/kmeansLoop.ipynb#W1sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mdone in \u001b[39m\u001b[39m%0.3f\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (time() \u001b[39m-\u001b[39m t0))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lauri/OneDrive%20-%20Sheffield%20Hallam%20University/Research/Clustering/Python/kmeansLoop.ipynb#W1sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m v \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mv_measure_score(labels, km\u001b[39m.\u001b[39mlabels_)\n",
      "File \u001b[1;32mc:\\Users\\lauri\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1146\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1112\u001b[0m \u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[0;32m   1113\u001b[0m \n\u001b[0;32m   1114\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1135\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m   1138\u001b[0m     X,\n\u001b[0;32m   1139\u001b[0m     accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1143\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m   1144\u001b[0m )\n\u001b[1;32m-> 1146\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_params(X)\n\u001b[0;32m   1147\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[0;32m   1148\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\lauri\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:937\u001b[0m, in \u001b[0;36mKMeans._check_params\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_params\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    936\u001b[0m     \u001b[39m# n_init\u001b[39;00m\n\u001b[1;32m--> 937\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_init \u001b[39m<\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m:\n\u001b[0;32m    938\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_init should be > 0, got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_init\u001b[39m}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    939\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_init \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_init\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "totalV = 0\n",
    "runs= 3\n",
    "for i in range(runs):\n",
    "  print(\"tfidf vectorizer\")\n",
    "  vectorizer = TfidfVectorizer(\n",
    "            max_df=0.5,\n",
    "            max_features= 2000, #10000,\n",
    "            min_df=2,\n",
    "            stop_words=\"english\",\n",
    "            use_idf= True\n",
    "  )\n",
    "  X = vectorizer.fit_transform(dataset.data)\n",
    "  print(\"done in %fs\" % (time() - t0))\n",
    "  print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "  numDocs = X.shape[0]   \n",
    "  km = KMeans(\n",
    "            n_clusters=true_k,\n",
    "            init=\"k-means++\",\n",
    "            max_iter=100,\n",
    "            n_init='auto',\n",
    "            verbose= False  \n",
    "        )\n",
    "\n",
    "  print(\"kMeans ++ run number: \" + str(i) + \"===============================================\")\n",
    "  print(\"Clustering sparse data with %s\" % km)\n",
    "  t0 = time()\n",
    "  km.fit(X)\n",
    "  print(\"done in %0.3fs\" % (time() - t0))\n",
    "  v = metrics.v_measure_score(labels, km.labels_)\n",
    "  h = metrics.homogeneity_score(labels, km.labels_)\n",
    "  c = metrics.completeness_score(labels, km.labels_)\n",
    "  adjustedRand = metrics.adjusted_rand_score(labels, km.labels_)\n",
    "\n",
    "  totalV = totalV + v\n",
    "\n",
    "  print(\"V-measure: %0.3f\" % v)\n",
    "  print(\"Homogeneity: %0.3f\" % h)\n",
    "  print(\"Completeness: %0.3f\" % c)\n",
    "  print(\"Adjusted Rand: %0.3f\" % adjustedRand)\n",
    "  print(\"===================================================================================\")\n",
    "  print()\n",
    "  \n",
    "  filePath = \"resultsKmeans.csv\"\n",
    "  resultsFile = open(filePath, \"a\")\n",
    "\n",
    "  if os.path.getsize(filePath) == 0:\n",
    "    resultsFile.write(\"index, v, h, c, adjustRand, numDocs \\n\")\n",
    "\n",
    "  resultsFile.write(\"crisis3, \" + str(v) +  \", \" + str(h) + \", \" + str(c) +  \", \" + str(adjustedRand) + \", \" + str(numDocs) + \", run number \" + str(i) + \"\\n\")\n",
    "  resultsFile.close()\n",
    "print(\"totalV \", totalV )\n",
    "averageV = totalV /  runs\n",
    "\n",
    "print(\"runs \", runs)\n",
    "print(\"average v \", averageV)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
